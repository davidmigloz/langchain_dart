// coverage:ignore-file
// GENERATED CODE - DO NOT MODIFY BY HAND
// ignore_for_file: type=lint
// ignore_for_file: invalid_annotation_target
part of anthropic_a_i_schema;

// ==========================================
// CLASS: Message
// ==========================================

/// No Description
@freezed
class Message with _$Message {
  const Message._();

  /// Factory constructor for Message
  const factory Message({
    /// Unique object identifier.
    ///
    /// The format and length of IDs may change over time.
    required String id,

    /// Content generated by the model.
    ///
    /// This is an array of content blocks, each of which has a `type` that determines
    /// its shape. Currently, the only `type` in responses is `"text"`.
    ///
    /// Example:
    ///
    /// ```json
    /// [{ "type": "text", "text": "Hi, I'm Claude." }]
    /// ```
    ///
    /// If the request input `messages` ended with an `assistant` turn, then the
    /// response `content` will continue directly from that last turn. You can use this
    /// to constrain the model's output.
    ///
    /// For example, if the input `messages` were:
    ///
    /// ```json
    /// [
    ///   {
    ///     "role": "user",
    ///     "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
    ///   },
    ///   { "role": "assistant", "content": "The best answer is (" }
    /// ]
    /// ```
    ///
    /// Then the response `content` might be:
    ///
    /// ```json
    /// [{ "type": "text", "text": "B)" }]
    /// ```
    required List<TextBlock> content,

    /// The model that handled the request.
    required String model,

    /// Conversational role of the generated message.
    ///
    /// This will always be `"assistant"`.
    required MessageRole role,

    /// The reason that we stopped.
    ///
    /// This may be one the following values:
    ///
    /// - `"end_turn"`: the model reached a natural stopping point
    /// - `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
    /// - `"stop_sequence"`: one of your provided custom `stop_sequences` was generated
    ///
    /// In non-streaming mode this value is always non-null. In streaming mode, it is
    /// null in the `message_start` event and non-null otherwise.
    @JsonKey(
      name: 'stop_reason',
      unknownEnumValue: JsonKey.nullForUndefinedEnumValue,
    )
    required MessageStopReason? stopReason,

    /// Which custom stop sequence was generated, if any.
    ///
    /// This value will be a non-null string if one of your custom stop sequences was
    /// generated.
    @JsonKey(name: 'stop_sequence') required String? stopSequence,

    /// Object type.
    ///
    /// For Messages, this is always `"message"`.
    required MessageType type,

    /// Billing and rate-limit usage.
    ///
    /// Anthropic's API bills and rate-limits by token counts, as tokens represent the
    /// underlying cost to our systems.
    ///
    /// Under the hood, the API transforms requests into a format suitable for the
    /// model. The model's output then goes through a parsing stage before becoming an
    /// API response. As a result, the token counts in `usage` will not match one-to-one
    /// with the exact visible content of an API request or response.
    ///
    /// For example, `output_tokens` will be non-zero, even for an empty string response
    /// from Claude.
    required Usage usage,
  }) = _Message;

  /// Object construction from a JSON representation
  factory Message.fromJson(Map<String, dynamic> json) =>
      _$MessageFromJson(json);

  /// List of all property names of schema
  static const List<String> propertyNames = [
    'id',
    'content',
    'model',
    'role',
    'stop_reason',
    'stop_sequence',
    'type',
    'usage'
  ];

  /// Perform validations on the schema property values
  String? validateSchema() {
    return null;
  }

  /// Map representation of object (not serialized)
  Map<String, dynamic> toMap() {
    return {
      'id': id,
      'content': content,
      'model': model,
      'role': role,
      'stop_reason': stopReason,
      'stop_sequence': stopSequence,
      'type': type,
      'usage': usage,
    };
  }
}

// ==========================================
// ENUM: MessageRole
// ==========================================

/// Conversational role of the generated message.
///
/// This will always be `"assistant"`.
enum MessageRole {
  @JsonValue('assistant')
  assistant,
}

// ==========================================
// ENUM: MessageStopReason
// ==========================================

/// The reason that we stopped.
///
/// This may be one the following values:
///
/// - `"end_turn"`: the model reached a natural stopping point
/// - `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
/// - `"stop_sequence"`: one of your provided custom `stop_sequences` was generated
///
/// In non-streaming mode this value is always non-null. In streaming mode, it is
/// null in the `message_start` event and non-null otherwise.
enum MessageStopReason {
  @JsonValue('end_turn')
  endTurn,
  @JsonValue('max_tokens')
  maxTokens,
  @JsonValue('stop_sequence')
  stopSequence,
}

// ==========================================
// ENUM: MessageType
// ==========================================

/// Object type.
///
/// For Messages, this is always `"message"`.
enum MessageType {
  @JsonValue('message')
  message,
}
