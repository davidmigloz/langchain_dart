openapi: 3.0.0
info:
  title: Anthropic API
  version: "v1"
paths:
  /v1/messages:
    post:
      summary: Create a Message
      description: |
        Send a structured list of input messages with text and/or image content, and the
        model will generate the next message in the conversation.

        The Messages API can be used for either single queries or stateless multi-turn
        conversations.
      operationId: createMessage
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/MessageCreateParamsNonStreaming"
      parameters: []
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Message"
components:
  schemas:
    ContentBlock:
      type: object
      properties:
        text:
          type: string
        type:
          type: string
          enum:
            - text
      required:
        - text
        - type
    ContentBlockDeltaEvent:
      type: object
      properties:
        delta:
          $ref: "#/components/schemas/TextDelta"
        index:
          type: integer
        type:
          type: string
          enum:
            - content_block_delta
      required:
        - delta
        - index
        - type
    ContentBlockStartEvent:
      type: object
      properties:
        content_block:
          $ref: "#/components/schemas/TextBlock"
        index:
          type: integer
        type:
          type: string
          enum:
            - content_block_start
      required:
        - content_block
        - index
        - type
    ContentBlockStopEvent:
      type: object
      properties:
        index:
          type: integer
        type:
          type: string
          enum:
            - content_block_stop
      required:
        - index
        - type
    ImageBlockParam:
      type: object
      properties:
        source:
          $ref: "#/components/schemas/ImageBlockParam_Source"
        type:
          type: string
          enum:
            - image
      required:
        - source
        - type
    ImageBlockParam_Source:
      type: object
      properties:
        data:
          type: string
        media_type:
          type: string
          enum:
            - image/jpeg
            - image/png
            - image/gif
            - image/webp
        type:
          type: string
          enum:
            - base64
      required:
        - data
        - media_type
        - type
    Message:
      type: object
      properties:
        id:
          type: string
          description: |
            Unique object identifier.

            The format and length of IDs may change over time.
        content:
          type: array
          description: |
            Content generated by the model.

            This is an array of content blocks, each of which has a `type` that determines
            its shape. Currently, the only `type` in responses is `"text"`.

            Example:

            ```json
            [{ "type": "text", "text": "Hi, I'm Claude." }]
            ```

            If the request input `messages` ended with an `assistant` turn, then the
            response `content` will continue directly from that last turn. You can use this
            to constrain the model's output.

            For example, if the input `messages` were:

            ```json
            [
              {
                "role": "user",
                "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
              },
              { "role": "assistant", "content": "The best answer is (" }
            ]
            ```

            Then the response `content` might be:

            ```json
            [{ "type": "text", "text": "B)" }]
            ```
          items:
            $ref: "#/components/schemas/TextBlock"
        model:
          type: string
          description: The model that handled the request.
        role:
          type: string
          description: |
            Conversational role of the generated message.

            This will always be `"assistant"`.
          enum:
            - assistant
        stop_reason:
          type: string
          description: |
            The reason that we stopped.

            This may be one the following values:

            - `"end_turn"`: the model reached a natural stopping point
            - `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
            - `"stop_sequence"`: one of your provided custom `stop_sequences` was generated

            In non-streaming mode this value is always non-null. In streaming mode, it is
            null in the `message_start` event and non-null otherwise.
          enum:
            - end_turn
            - max_tokens
            - stop_sequence
          nullable: true
        stop_sequence:
          type: string
          description: |
            Which custom stop sequence was generated, if any.

            This value will be a non-null string if one of your custom stop sequences was
            generated.
          nullable: true
        type:
          type: string
          description: |
            Object type.

            For Messages, this is always `"message"`.
          enum:
            - message
        usage:
          $ref: "#/components/schemas/Usage"
          description: |
            Billing and rate-limit usage.

            Anthropic's API bills and rate-limits by token counts, as tokens represent the
            underlying cost to our systems.

            Under the hood, the API transforms requests into a format suitable for the
            model. The model's output then goes through a parsing stage before becoming an
            API response. As a result, the token counts in `usage` will not match one-to-one
            with the exact visible content of an API request or response.

            For example, `output_tokens` will be non-zero, even for an empty string response
            from Claude.
      required:
        - id
        - content
        - model
        - role
        - stop_reason
        - stop_sequence
        - type
        - usage
    MessageDeltaEvent:
      type: object
      properties:
        delta:
          $ref: "#/components/schemas/MessageDeltaEvent_Delta"
        type:
          type: string
          enum:
            - message_delta
        usage:
          $ref: "#/components/schemas/MessageDeltaUsage"
          description: |
            Billing and rate-limit usage.

            Anthropic's API bills and rate-limits by token counts, as tokens represent the
            underlying cost to our systems.

            Under the hood, the API transforms requests into a format suitable for the
            model. The model's output then goes through a parsing stage before becoming an
            API response. As a result, the token counts in `usage` will not match one-to-one
            with the exact visible content of an API request or response.

            For example, `output_tokens` will be non-zero, even for an empty string response
            from Claude.
      required:
        - delta
        - type
        - usage
    MessageDeltaEvent_Delta:
      type: object
      properties:
        stop_reason:
          type: string
          enum:
            - end_turn
            - max_tokens
            - stop_sequence
          nullable: true
        stop_sequence:
          type: string
          nullable: true
      required:
        - stop_reason
        - stop_sequence
    MessageDeltaUsage:
      type: object
      properties:
        output_tokens:
          type: integer
          description: The cumulative number of output tokens which were used.
      required:
        - output_tokens
    MessageParam:
      type: object
      properties:
        content:
          anyOf:
            - type: string
            - type: array
              items:
                oneOf:
                  - $ref: "#/components/schemas/TextBlockParam"
                  - $ref: "#/components/schemas/ImageBlockParam"
        role:
          type: string
          enum:
            - user
            - assistant
      required:
        - content
        - role
    MessageStartEvent:
      type: object
      properties:
        message:
          $ref: "#/components/schemas/Message"
        type:
          type: string
          enum:
            - message_start
      required:
        - message
        - type
    MessageStopEvent:
      type: object
      properties:
        type:
          type: string
          enum:
            - message_stop
      required:
        - type
    MessageStreamEvent:
      oneOf:
        - $ref: "#/components/schemas/MessageStartEvent"
        - $ref: "#/components/schemas/MessageDeltaEvent"
        - $ref: "#/components/schemas/MessageStopEvent"
        - $ref: "#/components/schemas/ContentBlockStartEvent"
        - $ref: "#/components/schemas/ContentBlockDeltaEvent"
        - $ref: "#/components/schemas/ContentBlockStopEvent"
    TextBlock:
      type: object
      properties:
        text:
          type: string
        type:
          type: string
          enum:
            - text
      required:
        - text
        - type
    TextBlockParam:
      type: object
      properties:
        text:
          type: string
        type:
          type: string
          enum:
            - text
      required:
        - text
        - type
    TextDelta:
      type: object
      properties:
        text:
          type: string
        type:
          type: string
          enum:
            - text_delta
      required:
        - text
        - type
    Usage:
      type: object
      properties:
        input_tokens:
          type: integer
          description: The number of input tokens which were used.
        output_tokens:
          type: integer
          description: The number of output tokens which were used.
      required:
        - input_tokens
        - output_tokens
    MessageCreateParams:
      type: object
      properties:
        max_tokens:
          type: integer
          description: |
            The maximum number of tokens to generate before stopping.

            Note that our models may stop _before_ reaching this maximum. This parameter
            only specifies the absolute maximum number of tokens to generate.

            Different models have different maximum values for this parameter. See
            [models](https://docs.anthropic.com/en/docs/models-overview) for details.
        messages:
          type: array
          description: |
            Input messages.

            Our models are trained to operate on alternating `user` and `assistant`
            conversational turns. When creating a new `Message`, you specify the prior
            conversational turns with the `messages` parameter, and the model then generates
            the next `Message` in the conversation.

            Each input message must be an object with a `role` and `content`. You can
            specify a single `user`-role message, or you can include multiple `user` and
            `assistant` messages. The first message must always use the `user` role.

            If the final message uses the `assistant` role, the response content will
            continue immediately from the content in that message. This can be used to
            constrain part of the model's response.

            Example with a single `user` message:

            ```json
            [{ "role": "user", "content": "Hello, Claude" }]
            ```

            Example with multiple conversational turns:

            ```json
            [
              { "role": "user", "content": "Hello there." },
              { "role": "assistant", "content": "Hi, I'm Claude. How can I help you?" },
              { "role": "user", "content": "Can you explain LLMs in plain English?" }
            ]
            ```

            Example with a partially-filled response from Claude:

            ```json
            [
              {
                "role": "user",
                "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
              },
              { "role": "assistant", "content": "The best answer is (" }
            ]
            ```

            Each input message `content` may be either a single `string` or an array of
            content blocks, where each block has a specific `type`. Using a `string` for
            `content` is shorthand for an array of one content block of type `"text"`. The
            following input messages are equivalent:

            ```json
            { "role": "user", "content": "Hello, Claude" }
            ```

            ```json
            { "role": "user", "content": [{ "type": "text", "text": "Hello, Claude" }] }
            ```

            Starting with Claude 3 models, you can also send image content blocks:

            ```json
            {
              "role": "user",
              "content": [
                {
                  "type": "image",
                  "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": "/9j/4AAQSkZJRg..."
                  }
                },
                { "type": "text", "text": "What is in this image?" }
              ]
            }
            ```

            We currently support the `base64` source type for images, and the `image/jpeg`,
            `image/png`, `image/gif`, and `image/webp` media types.

            See [examples](https://docs.anthropic.com/en/api/messages-examples) for more
            input examples.

            Note that if you want to include a
            [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use
            the top-level `system` parameter — there is no `"system"` role for input
            messages in the Messages API.
          items:
            $ref: '#/components/schemas/MessageParam'
        model:
          type: string
          description: |
            The model that will complete your prompt.

            See [models](https://docs.anthropic.com/en/docs/models-overview) for additional
            details and options.
          enum:
            - claude-3-opus-20240229
            - claude-3-sonnet-20240229
            - claude-3-haiku-20240307
            - claude-2.1
            - claude-2.0
            - claude-instant-1.2
        metadata:
          $ref: '#/components/schemas/MessageCreateParams_Metadata'
          description: An object describing metadata about the request.
        stop_sequences:
          type: array
          description: |
            Custom text sequences that will cause the model to stop generating.

            Our models will normally stop when they have naturally completed their turn,
            which will result in a response `stop_reason` of `"end_turn"`.

            If you want the model to stop generating when it encounters custom strings of
            text, you can use the `stop_sequences` parameter. If the model encounters one of
            the custom sequences, the response `stop_reason` value will be `"stop_sequence"`
            and the response `stop_sequence` value will contain the matched stop sequence.
          items:
            type: string
        stream:
          type: boolean
          description: |
            Whether to incrementally stream the response using server-sent events.

            See [streaming](https://docs.anthropic.com/en/api/messages-streaming) for
            details.
        system:
          type: string
          description: |
            System prompt.

            A system prompt is a way of providing context and instructions to Claude, such
            as specifying a particular goal or role. See our
            [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
        temperature:
          type: number
          description: |
            Amount of randomness injected into the response.

            Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0`
            for analytical / multiple choice, and closer to `1.0` for creative and
            generative tasks.

            Note that even with `temperature` of `0.0`, the results will not be fully
            deterministic.
        top_k:
          type: integer
          description: |
            Only sample from the top K options for each subsequent token.

            Used to remove "long tail" low probability responses.
            [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).

            Recommended for advanced use cases only. You usually only need to use
            `temperature`.
        top_p:
          type: number
          description: |
            Use nucleus sampling.

            In nucleus sampling, we compute the cumulative distribution over all the options
            for each subsequent token in decreasing probability order and cut it off once it
            reaches a particular probability specified by `top_p`. You should either alter
            `temperature` or `top_p`, but not both.

            Recommended for advanced use cases only. You usually only need to use
            `temperature`.
      stream:
          type: boolean
          description: |
            Whether to incrementally stream the response using server-sent events.

            See [streaming](https://docs.anthropic.com/en/api/messages-streaming) for
            details.
          default: false
      required:
        - max_tokens
        - messages
        - model
        - stream
    MessageCreateParams_Metadata:
      type: object
      properties:
        user_id:
          type: string
          description: |
            An external identifier for the user who is associated with the request.

            This should be a uuid, hash value, or other opaque identifier. Anthropic may use
            this id to help detect abuse. Do not include any identifying information such as
            name, email address, or phone number.
          nullable: true
      required: []
    MessageCreateParamsNonStreaming:
      allOf:
        - $ref: "#/components/schemas/MessageCreateParams"
        - type: object
          properties:
            stream:
              type: boolean
              description: |
                Whether to incrementally stream the response using server-sent events.

                See [streaming](https://docs.anthropic.com/en/api/messages-streaming) for
                details.
              enum:
                - false
          required:
            - stream
    MessageCreateParamsStreaming:
      allOf:
        - $ref: "#/components/schemas/MessageCreateParams"
        - type: object
          properties:
            stream:
              type: boolean
              description: |
                Whether to incrementally stream the response using server-sent events.

                See [streaming](https://docs.anthropic.com/en/api/messages-streaming) for
                details.
              enum:
                - true
          required:
            - stream
    MessageStreamParams:
      type: object
      properties:
        max_tokens:
          type: integer
          description: |
            The maximum number of tokens to generate before stopping.

            Note that our models may stop _before_ reaching this maximum. This parameter
            only specifies the absolute maximum number of tokens to generate.

            Different models have different maximum values for this parameter. See
            [models](https://docs.anthropic.com/en/docs/models-overview) for details.
        messages:
          type: array
          description: |
            Input messages.

            Our models are trained to operate on alternating `user` and `assistant`
            conversational turns. When creating a new `Message`, you specify the prior
            conversational turns with the `messages` parameter, and the model then generates
            the next `Message` in the conversation.

            Each input message must be an object with a `role` and `content`. You can
            specify a single `user`-role message, or you can include multiple `user` and
            `assistant` messages. The first message must always use the `user` role.

            If the final message uses the `assistant` role, the response content will
            continue immediately from the content in that message. This can be used to
            constrain part of the model's response.

            Example with a single `user` message:

            ```json
            [{ "role": "user", "content": "Hello, Claude" }]
            ```

            Example with multiple conversational turns:

            ```json
            [
              { "role": "user", "content": "Hello there." },
              { "role": "assistant", "content": "Hi, I'm Claude. How can I help you?" },
              { "role": "user", "content": "Can you explain LLMs in plain English?" }
            ]
            ```

            Example with a partially-filled response from Claude:

            ```json
            [
              {
                "role": "user",
                "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
              },
              { "role": "assistant", "content": "The best answer is (" }
            ]
            ```

            Each input message `content` may be either a single `string` or an array of
            content blocks, where each block has a specific `type`. Using a `string` for
            `content` is shorthand for an array of one content block of type `"text"`. The
            following input messages are equivalent:

            ```json
            { "role": "user", "content": "Hello, Claude" }
            ```

            ```json
            { "role": "user", "content": [{ "type": "text", "text": "Hello, Claude" }] }
            ```

            Starting with Claude 3 models, you can also send image content blocks:

            ```json
            {
              "role": "user",
              "content": [
                {
                  "type": "image",
                  "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": "/9j/4AAQSkZJRg..."
                  }
                },
                { "type": "text", "text": "What is in this image?" }
              ]
            }
            ```

            We currently support the `base64` source type for images, and the `image/jpeg`,
            `image/png`, `image/gif`, and `image/webp` media types.

            See [examples](https://docs.anthropic.com/en/api/messages-examples) for more
            input examples.

            Note that if you want to include a
            [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use
            the top-level `system` parameter — there is no `"system"` role for input
            messages in the Messages API.
          items:
            $ref: "#/components/schemas/MessageParam"
        model:
          type: string
          description: |
            The model that will complete your prompt.

            See [models](https://docs.anthropic.com/en/docs/models-overview) for additional
            details and options.
          enum:
            - claude-3-opus-20240229
            - claude-3-sonnet-20240229
            - claude-3-haiku-20240307
            - claude-2.1
            - claude-2.0
            - claude-instant-1.2
        metadata:
          $ref: "#/components/schemas/MessageStreamParams_Metadata"
          description: An object describing metadata about the request.
        stop_sequences:
          type: array
          description: |
            Custom text sequences that will cause the model to stop generating.

            Our models will normally stop when they have naturally completed their turn,
            which will result in a response `stop_reason` of `"end_turn"`.

            If you want the model to stop generating when it encounters custom strings of
            text, you can use the `stop_sequences` parameter. If the model encounters one of
            the custom sequences, the response `stop_reason` value will be `"stop_sequence"`
            and the response `stop_sequence` value will contain the matched stop sequence.
          items:
            type: string
        system:
          type: string
          description: |
            System prompt.

            A system prompt is a way of providing context and instructions to Claude, such
            as specifying a particular goal or role. See our
            [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
        temperature:
          type: number
          description: |
            Amount of randomness injected into the response.

            Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0`
            for analytical / multiple choice, and closer to `1.0` for creative and
            generative tasks.

            Note that even with `temperature` of `0.0`, the results will not be fully
            deterministic.
        top_k:
          type: integer
          description: |
            Only sample from the top K options for each subsequent token.

            Used to remove "long tail" low probability responses.
            [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).

            Recommended for advanced use cases only. You usually only need to use
            `temperature`.
        top_p:
          type: number
          description: |
            Use nucleus sampling.

            In nucleus sampling, we compute the cumulative distribution over all the options
            for each subsequent token in decreasing probability order and cut it off once it
            reaches a particular probability specified by `top_p`. You should either alter
            `temperature` or `top_p`, but not both.

            Recommended for advanced use cases only. You usually only need to use
            `temperature`.
      required:
        - max_tokens
        - messages
        - model
    MessageStreamParams_Metadata:
      type: object
      properties:
        user_id:
          type: string
          description: |
            An external identifier for the user who is associated with the request.

            This should be a uuid, hash value, or other opaque identifier. Anthropic may use
            this id to help detect abuse. Do not include any identifying information such as
            name, email address, or phone number.
          nullable: true
      required: []
