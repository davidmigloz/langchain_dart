#file: noinspection YAMLUnusedAnchor
openapi: 3.0.0

info:
  title: OpenAI API
  description: The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
  version: "2.0.0"
  termsOfService: https://openai.com/policies/terms-of-use
  contact:
    name: OpenAI Support
    url: https://help.openai.com/
  license:
    name: MIT
    url: https://github.com/openai/openai-openapi/blob/master/LICENSE

servers:
  - url: https://api.openai.com/v1

tags:
  - name: Chat
    description: Given a list of messages comprising a conversation, the model will return a response.
  - name: Completions
    description: Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.
  - name: Embeddings
    description: Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.
  - name: Fine-tuning
    description: Manage fine-tuning jobs to tailor a model to your specific training data.
  - name: Images
    description: Given a prompt and/or an input image, the model will generate a new image.
  - name: Models
    description: List and describe the various models available in the API.
  - name: Moderations
    description: Given a input text, outputs if the model classifies it as violating OpenAI's content policy.

paths:
  /chat/completions:
    post:
      operationId: createChatCompletion
      tags:
        - Chat
      summary: Creates a model response for the given chat conversation.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateChatCompletionRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionResponse"
  /completions:
    post:
      operationId: createCompletion
      tags:
        - Completions
      summary: Creates a completion for the provided prompt and parameters.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateCompletionRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateCompletionResponse"
  /embeddings:
    post:
      operationId: createEmbedding
      tags:
        - Embeddings
      summary: Creates an embedding vector representing the input text.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateEmbeddingRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateEmbeddingResponse"
  /fine_tuning/jobs:
    post:
      operationId: createFineTuningJob
      tags:
        - Fine-tuning
      summary: |
        Creates a job that fine-tunes a specified model from a given dataset.

        Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.

        [Learn more about fine-tuning](https://platform.openai.com/docs/guides/fine-tuning).
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateFineTuningJobRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/FineTuningJob"
    get:
      operationId: listPaginatedFineTuningJobs
      tags:
        - Fine-tuning
      summary: |
        List your organization's fine-tuning jobs.
      parameters:
        - name: after
          in: query
          description: Identifier for the last job from the previous pagination request.
          required: false
          schema:
            type: string
        - name: limit
          in: query
          description: Number of fine-tuning jobs to retrieve.
          required: false
          schema:
            type: integer
            default: 20
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListPaginatedFineTuningJobsResponse"
  /fine_tuning/jobs/{fine_tuning_job_id}:
    get:
      operationId: retrieveFineTuningJob
      tags:
        - Fine-tuning
      summary: |
        Get info about a fine-tuning job.

        [Learn more about fine-tuning](https://platform.openai.com/docs/guides/fine-tuning).
      parameters:
        - in: path
          name: fine_tuning_job_id
          required: true
          schema:
            type: string
            example: ft-AF1WoRqd3aJAHsqc9NY7iL8F
          description: |
            The ID of the fine-tuning job.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/FineTuningJob"
  /fine_tuning/jobs/{fine_tuning_job_id}/events:
    get:
      operationId: listFineTuningEvents
      tags:
        - Fine-tuning
      summary: |
        Get status updates for a fine-tuning job.
      parameters:
        - in: path
          name: fine_tuning_job_id
          required: true
          schema:
            type: string
            example: ft-AF1WoRqd3aJAHsqc9NY7iL8F
          description: |
            The ID of the fine-tuning job to get events for.
        - name: after
          in: query
          description: Identifier for the last event from the previous pagination request.
          required: false
          schema:
            type: string
        - name: limit
          in: query
          description: Number of events to retrieve.
          required: false
          schema:
            type: integer
            default: 20
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListFineTuningJobEventsResponse"
  /fine_tuning/jobs/{fine_tuning_job_id}/cancel:
    post:
      operationId: cancelFineTuningJob
      tags:
        - Fine-tuning
      summary: |
        Immediately cancel a fine-tune job.
      parameters:
        - in: path
          name: fine_tuning_job_id
          required: true
          schema:
            type: string
            example: ft-AF1WoRqd3aJAHsqc9NY7iL8F
          description: |
            The ID of the fine-tuning job to cancel.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/FineTuningJob"
  /images/generations:
    post:
      operationId: createImage
      tags:
        - Images
      summary: Creates an image given a prompt.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateImageRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ImagesResponse"
  /models:
    get:
      operationId: listModels
      tags:
        - Models
      summary: Lists the currently available models, and provides basic information about each one such as the owner and availability.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListModelsResponse"
  /models/{model}:
    get:
      operationId: retrieveModel
      tags:
        - Models
      summary: Retrieves a model instance, providing basic information about the model such as the owner and permissioning.
      parameters:
        - in: path
          name: model
          required: true
          schema:
            type: string
            # ideally this will be an actual ID, so this will always work from browser
            example: gpt-3.5-turbo
          description: The ID of the model to use for this request
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Model"
    delete:
      operationId: deleteModel
      tags:
        - Models
      summary: Delete a fine-tuned model. You must have the Owner role in your organization to delete a model.
      parameters:
        - in: path
          name: model
          required: true
          schema:
            type: string
            example: ft:gpt-3.5-turbo:acemeco:suffix:abc123
          description: The model to delete
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteModelResponse"
  /moderations:
    post:
      operationId: createModeration
      tags:
        - Moderations
      summary: Classifies if text violates OpenAI's Content Policy.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateModerationRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateModerationResponse"
components:

  securitySchemes:
    ApiKeyAuth:
      type: http
      scheme: 'bearer'

  schemas:
    CreateCompletionRequest:
      type: object
      description: Request object for the Create completion endpoint.
      properties:
        model:
          title: CompletionModel
          description: &model_description |
            ID of the model to use. You can use the [List models](https://platform.openai.com/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](https://platform.openai.com/docs/models/overview) for descriptions of them.
          anyOf:
            - type: string
              description: The ID of the model to use for this request.
            - type: string
              title: CompletionModels
              description: |
                Available completion models. Mind that the list may not be exhaustive nor up-to-date.
              enum:
                [
                  "babbage-002",
                  "davinci-002",
                  "gpt-3.5-turbo-instruct",
                  "text-davinci-003",
                  "text-davinci-002",
                  "text-davinci-001",
                  "code-davinci-002",
                  "text-curie-001",
                  "text-babbage-001",
                  "text-ada-001",
                ]
        prompt:
          title: CompletionPrompt
          description: &completions_prompt_description |
            The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.

            Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
          default: "<|endoftext|>"
          nullable: true
          oneOf:
            - type: string
              description: A string prompt.
              default: ""
              example: "This is a test."
            - type: array
              description: A list of string prompts.
              items:
                type: string
                default: ""
                example: "This is a test."
            - type: array
              description: A tokenized prompt.
              minItems: 1
              items:
                type: integer
              example: "[1212, 318, 257, 1332, 13]"
            - type: array
              description: A list of tokenized prompts.
              minItems: 1
              items:
                type: array
                minItems: 1
                items:
                  type: integer
              example: "[[1212, 318, 257, 1332, 13]]"
        best_of:
          type: integer
          # default: 1 # Creates issues
          minimum: 0
          maximum: 20
          nullable: true
          description: &completions_best_of_description |
            Generates `best_of` completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.
            
            When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.

            **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        echo:
          type: boolean
          default: false
          nullable: true
          description: &completions_echo_description >
            Echo back the prompt in addition to the completion
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: &completions_frequency_penalty_description |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

            [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)
        logit_bias: &completions_logit_bias
          type: object
          default: null
          nullable: true
          additionalProperties:
            type: integer
          description: &completions_logit_bias_description |
            Modify the likelihood of specified tokens appearing in the completion.

            Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](https://platform.openai.com/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

            As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.
        logprobs: &completions_logprobs_configuration
          type: integer
          minimum: 0
          maximum: 5
          default: null
          nullable: true
          description: &completions_logprobs_description |
            Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.

            The maximum value for `logprobs` is 5.
        max_tokens:
          type: integer
          minimum: 0
          default: 16
          example: 16
          nullable: true
          description: &completions_max_tokens_description |
            The maximum number of [tokens](https://platform.openai.com/tokenizer) to generate in the completion.

            The token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
        n:
          type: integer
          minimum: 1
          maximum: 128
          default: 1
          example: 1
          nullable: true
          description: &completions_completions_description |
            How many completions to generate for each prompt.

            **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: &completions_presence_penalty_description |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

            [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)
        seed: &completions_seed_param
          type: integer
          # minimum: -9223372036854775808 # The value can't be represented exactly in JavaScript
          # maximum: 9223372036854775807
          nullable: true
          description: |
            If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.

            Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
        stop:
          title: CompletionStop
          description: &completions_stop_description >
            Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
          default: null
          nullable: true
          oneOf:
            - type: string
              description: A string stop sequence.
              default: <|endoftext|>
              example: "\n"
              nullable: true
            - type: array
              description: A list of string stop sequences.
              minItems: 1
              maxItems: 4
              items:
                type: string
                example: '["\n"]'
        stream:
          description: >
            Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
          type: boolean
          nullable: true
          default: false
        suffix:
          description: The suffix that comes after a completion of inserted text.
          default: null
          nullable: true
          type: string
          example: "test."
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: &completions_temperature_description |
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: &completions_top_p_description |
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.

            We generally recommend altering this or `temperature` but not both.
        user: &end_user_param_configuration
          type: string
          example: user-1234
          description: |
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).
      required:
        - model
        - prompt
    CreateCompletionResponse:
      type: object
      description: |
        Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).
      properties:
        id:
          type: string
          description: A unique identifier for the completion.
        choices:
          type: array
          description: The list of completion choices the model generated for the input prompt.
          items:
            $ref: "#/components/schemas/CompletionChoice"
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the completion was created.
        model:
          type: string
          description: The model used for completion.
        system_fingerprint:
          type: string
          description: |
            This fingerprint represents the backend configuration that the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
        object:
          type: string
          description: The object type, which is always "text_completion"
          enum: [ text_completion ]
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
        - id
        - object
        - created
        - model
        - choices
    CompletionChoice:
      type: object
      description: A choice the model generated for the input prompt.
      required:
        - finish_reason
        - index
        - logprobs
        - text
      properties:
        finish_reason:
          $ref: "#/components/schemas/CompletionFinishReason"
        index:
          type: integer
          description: The index of the choice in the list of generated choices.
        logprobs:
          $ref: "#/components/schemas/CompletionLogprobs"
          nullable: true
        text:
          type: string
          description: The text of the completion.
    CompletionFinishReason:
      type: string
      nullable: true
      description: &completion_finish_reason_description |
        The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
        `length` if the maximum number of tokens specified in the request was reached,
        or `content_filter` if content was omitted due to a flag from our content filters.
      enum: [ "stop", "length", "content_filter" ]
    CompletionLogprobs:
      type: object
      description: |
        The probabilities on the `logprobs` most likely tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.
      properties:
        text_offset:
          type: array
          description: The offset of the token from the beginning of the prompt.
          items:
            type: integer
        token_logprobs:
          type: array
          description: The log probabilities of tokens in the completion.
          items:
            type: number
            nullable: true
        tokens:
          type: array
          description: The tokens generated by the model converted back to text.
          items:
            type: string
        top_logprobs:
          type: array
          description: The log probabilities of the `logprobs` most likely tokens.
          items:
            type: object
            nullable: true
            additionalProperties:
              type: number
    CreateChatCompletionRequest:
      type: object
      description: Request object for the Create chat completion endpoint.
      properties:
        model:
          title: ChatCompletionModel
          description: ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
          example: "gpt-3.5-turbo"
          anyOf:
            - type: string
              description: The ID of the model to use for this request.
            - type: string
              title: ChatCompletionModels
              description: |
                Available completion models. Mind that the list may not be exhaustive nor up-to-date.
              enum:
                [
                  "gpt-4",
                  "gpt-4-0314",
                  "gpt-4-0613",
                  "gpt-4-32k",
                  "gpt-4-32k-0314",
                  "gpt-4-32k-0613",
                  "gpt-4-1106-preview",
                  "gpt-4-vision-preview",
                  "gpt-3.5-turbo",
                  "gpt-3.5-turbo-16k",
                  "gpt-3.5-turbo-0301",
                  "gpt-3.5-turbo-0613",
                  "gpt-3.5-turbo-1106",
                  "gpt-3.5-turbo-16k-0613",
                ]
        messages:
          description: A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
          type: array
          minItems: 1
          items:
            $ref: "#/components/schemas/ChatCompletionMessage"
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: *completions_frequency_penalty_description
        logit_bias:
          type: object
          default: null
          nullable: true
          additionalProperties:
            type: integer
          description: |
            Modify the likelihood of specified tokens appearing in the completion.

            Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        max_tokens:
          description: |
            The maximum number of [tokens](https://platform.openai.com/tokenizer) to generate in the chat completion.

            The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
          default: inf
          type: integer
          nullable: true
        n:
          type: integer
          minimum: 1
          maximum: 128
          default: 1
          example: 1
          nullable: true
          description: How many chat completion choices to generate for each input message.
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: *completions_presence_penalty_description
        response_format:
          title: ChatCompletionResponseFormat
          type: object
          description: |
            An object specifying the format that the model must output. 

            Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.

            **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
          properties:
            type:
              type: string
              enum: [ "text", "json_object" ]
              example: "json_object"
              default: "text"
              description: Must be one of `text` or `json_object`.
        seed:
          type: integer
          # minimum: -9223372036854775808 # The value can't be represented exactly in JavaScript
          # maximum: 9223372036854775807
          nullable: true
          description: |
            This feature is in Beta. 
            If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
            Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
        stop:
          title: ChatCompletionStop
          description: |
            Up to 4 sequences where the API will stop generating further tokens.
          default: null
          oneOf:
            - type: string
              description: A string stop sequence.
              nullable: true
            - type: array
              description: A list of string stop sequences.
              minItems: 1
              maxItems: 4
              items:
                type: string
        stream:
          description: >
            If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
          type: boolean
          nullable: true
          default: false
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: *completions_temperature_description
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: *completions_top_p_description
        tools:
          type: array
          description: >
            A list of tools the model may call. Currently, only functions are supported as a tool.
            Use this to provide a list of functions the model may generate JSON inputs for.
          items:
            $ref: "#/components/schemas/ChatCompletionTool"
        tool_choice:
          title: ChatCompletionToolChoiceOption
          description: |
            Controls which (if any) function is called by the model.
            `none` means the model will not call a function and instead generates a message.
            `auto` means the model can pick between generating a message or calling a function.
            Specifying a particular function via `{"type: "function", "function": {"name": "my_function"}}` forces the model to call that function.
            
            `none` is the default when no functions are present. `auto` is the default if functions are present.
          oneOf:
            - type: string
              title: ChatCompletionToolChoiceMode
              description: >
                `none` means the model will not call a function and instead generates a message.
                `auto` means the model can pick between generating a message or calling a function.
              enum: [ none, auto ]
            - $ref: "#/components/schemas/ChatCompletionNamedToolChoice"
        user: *end_user_param_configuration
        function_call:
          title: ChatCompletionFunctionCall
          deprecated: true
          description: |
            Deprecated in favor of `tool_choice`.
            
            Controls which (if any) function is called by the model.
            `none` means the model will not call a function and instead generates a message.
            `auto` means the model can pick between generating a message or calling a function.
            Specifying a particular function via [ChatCompletionFunctionCallOption] forces the model to call that function.
            
            `none` is the default when no functions are present. `auto`` is the default if functions are present.
          oneOf:
            - type: string
              title: ChatCompletionFunctionCallMode
              description: >
                `none` means the model will not call a function and instead generates a message.
                `auto` means the model can pick between generating a message or calling a function.
              enum: [ none, auto ]
            - $ref: "#/components/schemas/ChatCompletionFunctionCallOption"
        functions:
          deprecated: true
          description: |
            Deprecated in favor of `tools`.

            A list of functions the model may generate JSON inputs for.
          type: array
          minItems: 1
          maxItems: 128
          items:
            $ref: "#/components/schemas/FunctionObject"
      required:
        - model
        - messages
    ChatCompletionMessage:
      type: object
      description: A message in a chat conversation.
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionSystemMessage"
        - $ref: "#/components/schemas/ChatCompletionUserMessage"
        - $ref: "#/components/schemas/ChatCompletionAssistantMessage"
        - $ref: "#/components/schemas/ChatCompletionToolMessage"
        - $ref: "#/components/schemas/ChatCompletionFunctionMessage"
      discriminator:
        propertyName: role
    ChatCompletionSystemMessage:
      type: object
      description: A system message in a chat conversation.
      properties:
        role:
          $ref: "#/components/schemas/ChatCompletionMessageRole"
          default: system
          description: The role of the messages author, in this case `system`.
        content:
          nullable: true
          description: The contents of the system message.
          type: string
        name:
          type: string
          description: An optional name for the participant. Provides the model information to differentiate between participants of the same role.
      required:
        - content
    ChatCompletionUserMessage:
      type: object
      description: A user message in a chat conversation.
      properties:
        role:
          $ref: "#/components/schemas/ChatCompletionMessageRole"
          default: user
          description: The role of the messages author, in this case `user`.
        content:
          # TODO extract to ChatCompletionMessageContent once generator bug fixed
          nullable: true
          description: The contents of the user message.
          oneOf:
            - type: string
              description: The text contents of the message.
            - type: array
              description: An array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Image input is only supported when using the `gpt-4-visual-preview` model.
              items:
                $ref: "#/components/schemas/ChatCompletionMessageContentPart"
              minItems: 1
        name:
          type: string
          description: An optional name for the participant. Provides the model information to differentiate between participants of the same role.
      required:
        - content
    ChatCompletionAssistantMessage:
      type: object
      description: An assistant message in a chat conversation.
      properties:
        role:
          $ref: "#/components/schemas/ChatCompletionMessageRole"
          default: assistant
          description: The role of the messages author, in this case `assistant`.
        content:
          nullable: true
          type: string
          description: |
            The contents of the assistant message.
        name:
          type: string
          description: An optional name for the participant. Provides the model information to differentiate between participants of the same role.
        tool_calls:
          $ref: "#/components/schemas/ChatCompletionMessageToolCalls"
          description: The tools that should be called, as generated by the model.
        function_call:
          $ref: "#/components/schemas/ChatCompletionMessageFunctionCall"
          deprecated: true
          description: "Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model."
      required:
        - content
    ChatCompletionToolMessage:
      type: object
      description: A tool message in a chat conversation.
      properties:
        role:
          $ref: "#/components/schemas/ChatCompletionMessageRole"
          default: tool
          description: The role of the messages author, in this case `tool`.
        content:
          nullable: true
          type: string
          description: The contents of the tool message.
        tool_call_id:
          type: string
          description: Tool call that this message is responding to.
      required:
        - content
        - tool_call_id
    ChatCompletionFunctionMessage:
      type: object
      description: A function message in a chat conversation.
      deprecated: true
      properties:
        role:
          $ref: "#/components/schemas/ChatCompletionMessageRole"
          default: function
          description: The role of the messages author, in this case `function`.
        content:
          type: string
          description: The return value from the function call, to return to the model.
        name:
          type: string
          description: The name of the function to call.
      required:
        - name
        - content
    ChatCompletionMessageContentPart:
      description: A content part of a user message.
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionMessageContentPartText"
        - $ref: "#/components/schemas/ChatCompletionMessageContentPartImage"
      discriminator:
        propertyName: type
    ChatCompletionMessageContentPartText:
      type: object
      description: A text content part of a user message.
      properties:
        type:
          $ref: "#/components/schemas/ChatCompletionMessageContentPartType"
          default: text
          description: The type of the content part, in this case `text`.
        text:
          type: string
          description: The text content.
      required:
        - text
    ChatCompletionMessageContentPartImage:
      type: object
      title: Image content part
      properties:
        type:
          $ref: "#/components/schemas/ChatCompletionMessageContentPartType"
          default: image_url
          description: The type of the content part, in this case `image_url`.
        image_url:
          title: ChatCompletionMessageImageUrl
          description: The URL of the image.
          type: object
          properties:
            url:
              type: string
              description: Either a URL of the image or the base64 encoded image data.
              format: uri
            detail:
              type: string
              title: ChatCompletionMessageImageDetail
              description: Specifies the detail level of the image. Learn more in the [Vision guide](https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding).
              enum: ["auto", "low", "high"]
              default: "auto"
          required:
            - url
      required:
        - image_url
    ChatCompletionMessageContentPartType:
      type: string
      enum: ["text", "image_url"]
      description: The type of the content part.
    ChatCompletionMessageRole:
      type: string
      description: The role of the messages author. One of `system`, `user`, `assistant`, or `tool` (`function` is deprecated).
      enum: [ "system", "user", "assistant", "tool", "function" ]
    ChatCompletionMessageFunctionCall:
      type: object
      description: The name and arguments of a function that should be called, as generated by the model.
      properties:
        name:
          type: string
          description: The name of the function to call.
        arguments:
          type: string
          description: The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
      required:
        - name
        - arguments
    ChatCompletionFunctionCallOption:
      type: object
      description: Forces the model to call the specified function.
      properties:
        name:
          type: string
          description: The name of the function to call.
      required:
        - name
    FunctionObject:
      type: object
      description: A function that the model may call.
      properties:
        name:
          type: string
          description: The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
        description:
          type: string
          description: A description of what the function does, used by the model to choose when and how to call the function.
        parameters:
          $ref: "#/components/schemas/FunctionParameters"
      required:
        - name
        - parameters
    FunctionParameters:
      type: object
      description: "The parameters the functions accepts, described as a JSON Schema object. See the [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.\n\nTo describe a function that accepts no parameters, provide the value `{\"type\": \"object\", \"properties\": {}}`."
      additionalProperties: true
    ChatCompletionTool:
      type: object
      description: A tool the model may use.
      properties:
        type:
          type: string
          enum: [ "function" ]
          description: The type of the tool. Currently, only `function` is supported.
        function:
          $ref: "#/components/schemas/FunctionObject"
      required:
        - type
        - function
    ChatCompletionNamedToolChoice:
      type: object
      description: Specifies a tool the model should use. Use to force the model to call a specific function.
      properties:
        type:
          type: string
          enum: [ "function" ]
          description: The type of the tool. Currently, only `function` is supported.
        function:
          $ref: "#/components/schemas/ChatCompletionFunctionCallOption"
    ChatCompletionMessageToolCalls:
      type: array
      description: The tool calls generated by the model, such as function calls.
      items:
        $ref: "#/components/schemas/ChatCompletionMessageToolCall"
    ChatCompletionMessageToolCall:
      type: object
      description: A tool call generated by the model, such as a function call.
      properties:
        # TODO: index included when streaming
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum: [ "function" ]
          description: The type of the tool. Currently, only `function` is supported.
        function:
          $ref: "#/components/schemas/ChatCompletionMessageFunctionCall"
      required:
        - id
        - type
        - function
    CreateChatCompletionResponse:
      type: object
      description: Represents a chat completion response returned by model, based on the provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        choices:
          type: array
          description: A list of chat completion choices. Can be more than one if `n` is greater than 1.
          items:
            $ref: "#/components/schemas/ChatCompletionResponseChoice"
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion was created.
        model:
          type: string
          description: The model used for the chat completion.
        system_fingerprint:
          type: string
          description: |
            This fingerprint represents the backend configuration that the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
        object:
          type: string
          description: The object type, which is always `chat.completion`.
          enum: [ chat.completion ]
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
        - choices
        - created
        - id
        - model
        - object
    ChatCompletionResponseChoice:
      type: object
      description: A choice the model generated for the input prompt.
      required:
        - finish_reason
        - index
        - message
      properties:
        finish_reason:
          $ref: "#/components/schemas/ChatCompletionFinishReason"
          nullable: true
          # TODO New models seems to use finish_details instead
        index:
          type: integer
          description: The index of the choice in the list of choices.
        message:
          $ref: "#/components/schemas/ChatCompletionAssistantMessage"
    ChatCompletionFinishReason:
      type: string
      description: &chat_completion_finish_reason_description |
        The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
        `length` if the maximum number of tokens specified in the request was reached,
        `content_filter` if content was omitted due to a flag from our content filters,
        `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
      enum:
        [
          "stop",
          "length",
          "tool_calls",
          "content_filter",
          "function_call",
        ]
    CreateChatCompletionStreamResponse:
      type: object
      description: Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion. Each chunk has the same ID.
        choices:
          type: array
          description: A list of chat completion choices. Can be more than one if `n` is greater than 1.
          items:
            $ref: "#/components/schemas/ChatCompletionStreamResponseChoice"
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
        model:
          type: string
          description: The model to generate the completion.
        system_fingerprint:
          type: string
          description: |
            This fingerprint represents the backend configuration that the model runs with.
            
            Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact
        object:
          type: string
          description: The object type, which is always `chat.completion.chunk`.
          enum: [ chat.completion.chunk ]
      required:
        - choices
        - created
        - id
        - model
        - object
    ChatCompletionStreamResponseChoice:
      type: object
      description: A choice the model generated for the input prompt.
      required:
        - delta
        - finish_reason
        - index
      properties:
        delta:
          $ref: "#/components/schemas/ChatCompletionStreamResponseDelta"
        finish_reason:
          $ref: "#/components/schemas/ChatCompletionFinishReason"
          nullable: true
        index:
          type: integer
          description: The index of the choice in the list of choices.
    ChatCompletionStreamResponseDelta:
      type: object
      description: A chat completion delta generated by streamed model responses.
      properties:
        content:
          type: string
          description: The contents of the chunk message.
          nullable: true
        function_call:
          $ref: "#/components/schemas/ChatCompletionStreamMessageFunctionCall"
        tool_calls:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionStreamMessageToolCallChunk"
        role:
          $ref: "#/components/schemas/ChatCompletionMessageRole"
    ChatCompletionStreamMessageFunctionCall:
      type: object
      description: The name and arguments of a function that should be called, as generated by the model.
      properties:
        name:
          type: string
          description: The name of the function to call.
        arguments:
          type: string
          description: The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    ChatCompletionStreamMessageToolCallChunk:
      type: object
      description: The tool that should be called, as generated by the model.
      properties:
        index:
          type: integer
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum: [ "function" ]
          description: The type of the tool. Currently, only `function` is supported.
        function:
          $ref: "#/components/schemas/ChatCompletionStreamMessageFunctionCall"
      required:
        - index
    CompletionUsage:
      type: object
      description: Usage statistics for the completion request.
      properties:
        completion_tokens:
          type: integer
          nullable: true
          description: Number of tokens in the generated completion.
        prompt_tokens:
          type: integer
          description: Number of tokens in the prompt.
        total_tokens:
          type: integer
          description: Total number of tokens used in the request (prompt + completion).
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
    CreateEmbeddingRequest:
      type: object
      description: Request object for the Create embedding endpoint.
      additionalProperties: false
      properties:
        model:
          title: EmbeddingModel
          description: *model_description
          example: "text-embedding-ada-002"
          anyOf:
            - type: string
              description: The ID of the model to use for this request.
            - type: string
              title: EmbeddingModels
              description: |
                Available completion models. Mind that the list may not be exhaustive nor up-to-date.
              enum: [ "text-embedding-ada-002" ]
        input:
          title: EmbeddingInput
          description: |
            Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for `text-embedding-ada-002`), cannot be an empty string, and any array must be 2048 dimensions or less. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
          example: "The quick brown fox jumped over the lazy dog"
          oneOf:
            - type: string
              description: A string prompt.
              default: ""
              example: "This is a test."
            - type: array
              description: A list of string prompts.
              minItems: 1
              maxItems: 2048
              items:
                type: string
                default: ""
                example: "This is a test."
            - type: array
              description: A tokenized prompt.
              minItems: 1
              maxItems: 2048
              items:
                type: integer
              example: "[1212, 318, 257, 1332, 13]"
            - type: array
              description: A list of tokenized prompts.
              minItems: 1
              maxItems: 2048
              items:
                type: array
                minItems: 1
                items:
                  type: integer
              example: "[[1212, 318, 257, 1332, 13]]"
        encoding_format:
          title: EmbeddingEncodingFormat
          description: "The format to return the embeddings in. Can be either `float` or [`base64`](https://pypi.org/project/pybase64/)."
          example: "float"
          default: "float"
          type: string
          enum: [ "float", "base64" ]
        user: *end_user_param_configuration
      required:
        - model
        - input
    CreateEmbeddingResponse:
      type: object
      description: Represents an embedding vector returned by embedding endpoint.
      properties:
        data:
          type: array
          description: The list of embeddings generated by the model.
          items:
            $ref: "#/components/schemas/Embedding"
        model:
          type: string
          description: The name of the model used to generate the embedding.
        object:
          type: string
          description: The object type, which is always "list".
          enum: [ list ]
        usage:
          $ref: "#/components/schemas/EmbeddingUsage"
      required:
        - object
        - model
        - data
        - usage
    Embedding:
      type: object
      description: |
        Represents an embedding vector returned by embedding endpoint.
      properties:
        index:
          type: integer
          description: The index of the embedding in the list of embeddings.
        embedding:
          title: EmbeddingVector
          description: |
            The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the [embedding guide](https://platform.openai.com/docs/guides/embeddings).
          oneOf:
            - type: string
              description: The embedding vector as a base64-encoded string.
            - type: array
              description: The embedding vector as a list of floats.
              items:
                type: number
        object:
          type: string
          description: The object type, which is always "embedding".
          enum: [ embedding ]
      required:
        - index
        - object
        - embedding
    EmbeddingUsage:
      type: object
      description: The usage information for the request.
      properties:
        prompt_tokens:
          type: integer
          description: The number of tokens used by the prompt.
        total_tokens:
          type: integer
          description: The total number of tokens used by the request.
      required:
        - prompt_tokens
        - total_tokens
    CreateFineTuningJobRequest:
      type: object
      description: Request object for the Create fine-tuning job endpoint.
      properties:
        model:
          title: FineTuningModel
          description: |
            The name of the model to fine-tune. You can select one of the
            [supported models](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned).
          example: "gpt-3.5-turbo"
          anyOf:
            - type: string
              description: The ID of the model to use for this request.
            - type: string
              title: FineTuningModels
              description: |
                Available fine-tuning models. Mind that the list may not be exhaustive nor up-to-date.
              enum: [ "babbage-002", "davinci-002", "gpt-3.5-turbo" ]
        training_file:
          description: |
            The ID of an uploaded file that contains training data.

            See [upload file](https://platform.openai.com/docs/api-reference/files/upload) for how to upload a file.

            Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with the purpose `fine-tune`.

            See the [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning) for more details.
          type: string
          example: "file-abc123"
        hyperparameters:
          $ref: "#/components/schemas/FineTuningJobHyperparameters"
        suffix:
          description: |
            A string of up to 18 characters that will be added to your fine-tuned model name.

            For example, a `suffix` of "custom-model-name" would produce a model name like `ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel`.
          type: string
          minLength: 1
          maxLength: 40
          default: null
          nullable: true
        validation_file:
          description: |
            The ID of an uploaded file that contains validation data.

            If you provide this file, the data is used to generate validation
            metrics periodically during fine-tuning. These metrics can be viewed in
            the fine-tuning results file.
            The same data should not be present in both train and validation files.

            Your dataset must be formatted as a JSONL file. You must upload your file with the purpose `fine-tune`.

            See the [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning) for more details.
          type: string
          nullable: true
          example: "file-abc123"
      required:
        - model
        - training_file
    FineTuningJob:
      type: object
      title: FineTuningJob
      description: |
        The `fine_tuning.job` object represents a fine-tuning job that has been created through the API.
      properties:
        id:
          type: string
          description: The object identifier, which can be referenced in the API endpoints.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the fine-tuning job was created.
        error:
          $ref: "#/components/schemas/FineTuningJobError"
        fine_tuned_model:
          type: string
          nullable: true
          description: The name of the fine-tuned model that is being created. The value will be null if the fine-tuning job is still running.
        finished_at:
          type: integer
          nullable: true
          description: The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be null if the fine-tuning job is still running.
        hyperparameters:
          $ref: "#/components/schemas/FineTuningJobHyperparameters"
        model:
          type: string
          description: The base model that is being fine-tuned.
        object:
          type: string
          description: The object type, which is always "fine_tuning.job".
          enum: [ fine_tuning.job ]
        organization_id:
          type: string
          description: The organization that owns the fine-tuning job.
        result_files:
          type: array
          description: The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
          items:
            type: string
            example: file-abc123
        status:
          $ref: "#/components/schemas/FineTuningJobStatus"
        trained_tokens:
          type: integer
          nullable: true
          description: The total number of billable tokens processed by this fine-tuning job. The value will be null if the fine-tuning job is still running.
        training_file:
          type: string
          description: The file ID used for training. You can retrieve the training data with the [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
        validation_file:
          type: string
          nullable: true
          description: The file ID used for validation. You can retrieve the validation results with the [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).
      required:
        - created_at
        - error
        - finished_at
        - fine_tuned_model
        - hyperparameters
        - id
        - model
        - object
        - organization_id
        - result_files
        - status
        - trained_tokens
        - training_file
        - validation_file
    FineTuningJobStatus:
      type: string
      description: The current status of the fine-tuning job, which can be either `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.
      enum:
        [
          "validating_files",
          "queued",
          "running",
          "succeeded",
          "failed",
          "cancelled",
        ]
    FineTuningJobError:
      type: object
      nullable: true
      description: For fine-tuning jobs that have `failed`, this will contain more information on the cause of the failure.
      properties:
        code:
          type: string
          description: A machine-readable error code.
        message:
          type: string
          description: A human-readable error message.
        param:
          type: string
          description: The parameter that was invalid, usually `training_file` or `validation_file`. This field will be null if the failure was not parameter-specific.
          nullable: true
      required:
        - code
        - message
        - param
    FineTuningJobHyperparameters:
      type: object
      description: The hyperparameters used for the fine-tuning job. See the [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning) for more details.
      properties:
        n_epochs:
          title: FineTuningNEpochs
          description: |
            The number of epochs to train the model for. An epoch refers to one
            full cycle through the training dataset.
          oneOf:
            - type: string
              title: FineTuningNEpochsOptions
              description: The mode for the number of epochs.
              enum: [ auto ]
            - type: integer
              description: The number of epochs to train the model for.
              minimum: 1
              maximum: 50
          default: auto
      required:
        - n_epochs
    ListPaginatedFineTuningJobsResponse:
      type: object
      description: Represents a list of fine-tuning jobs.
      properties:
        data:
          type: array
          description: The list of fine-tuning jobs.
          items:
            $ref: "#/components/schemas/FineTuningJob"
        has_more:
          type: boolean
          description: Whether there are more fine-tuning jobs to retrieve.
        object:
          type: string
          description: The object type, which is always "list".
          enum: [ list ]
      required:
        - object
        - data
        - has_more
    ListFineTuningJobEventsResponse:
      type: object
      description: Represents a list of fine-tuning job events.
      properties:
        data:
          type: array
          description: The list of fine-tuning job events.
          items:
            $ref: "#/components/schemas/FineTuningJobEvent"
        object:
          type: string
          description: The object type, which is always "list".
          enum: [ list ]
      required:
        - object
        - data
    FineTuningJobEvent:
      type: object
      description: Fine-tuning job event object.
      properties:
        id:
          type: string
          description: The event identifier, which can be referenced in the API endpoints.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the event was created.
        level:
          type: string
          description: The log level of the event.
          enum: [ "info", "warn", "error" ]
        message:
          type: string
          description: The message of the event.
        object:
          type: string
          description: The object type, which is always "fine_tuning.job.event".
          enum: [fine_tuning.job.event]
      required:
        - id
        - object
        - created_at
        - level
        - message
    CreateImageRequest:
      type: object
      description: Request object for the Create image endpoint.
      properties:
        prompt:
          description: A text description of the desired image(s). The maximum length is 1000 characters for `dall-e-2` and 4000 characters for `dall-e-3`.
          type: string
          example: "A cute baby sea otter"
        model:
          anyOf:
            - type: string
              description: The ID of the model to use for this request.
            - type: string
              title: ImageModels
              description: |
                Available models for image generation. Mind that the list may not be exhaustive nor up-to-date.
              enum: ["dall-e-2", "dall-e-3"]
          default: "dall-e-2"
          example: "dall-e-3"
          nullable: true
          description: The model to use for image generation.
        n: &images_n
          type: integer
          minimum: 1
          maximum: 10
          default: 1
          example: 1
          nullable: true
          description: The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only `n=1` is supported.
        quality:
          title: ImageQuality
          type: string
          enum: ["standard", "hd"]
          default: "standard"
          example: "standard"
          description: The quality of the image that will be generated. `hd` creates images with finer details and greater consistency across the image. This param is only supported for `dall-e-3`.
        response_format: &images_response_format
          title: ImageResponseFormat
          type: string
          enum: [ "url", "b64_json" ]
          default: "url"
          example: "url"
          nullable: true
          description: The format in which the generated images are returned. Must be one of `url` or `b64_json`.
        size: &images_size
          title: ImageSize
          type: string
          enum: [ "256x256", "512x512", "1024x1024", "1792x1024", "1024x1792" ]
          default: "1024x1024"
          example: "1024x1024"
          nullable: true
          description: The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models.
        style:
          title: ImageStyle
          type: string
          enum: ["vivid", "natural"]
          default: "vivid"
          example: "vivid"
          nullable: true
          description: The style of the generated images. Must be one of `vivid` or `natural`. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for `dall-e-3`.
        user: *end_user_param_configuration
      required:
        - prompt
    ImagesResponse:
      type: object
      description: Represents a generated image returned by the images endpoint.
      properties:
        created:
          type: integer
          description: The Unix timestamp (in seconds) when the image was created.
        data:
          type: array
          description: The list of images generated by the model.
          items:
            $ref: "#/components/schemas/Image"
      required:
        - created
        - data
    Image:
      type: object
      description: Represents the url or the content of an image generated by the OpenAI API.
      properties:
        b64_json:
          type: string
          description: The base64-encoded JSON of the generated image, if `response_format` is `b64_json`.
        url:
          type: string
          description: The URL of the generated image, if `response_format` is `url` (default).
        revised_prompt:
          type: string
          description: The prompt that was used to generate the image, if there was any revision to the prompt.
    Model:
      title: Model
      description: Describes an OpenAI model offering that can be used with the API.
      properties:
        id:
          type: string
          description: The model identifier, which can be referenced in the API endpoints.
        created:
          type: integer
          description: The Unix timestamp (in seconds) when the model was created.
        object:
          type: string
          description: The object type, which is always "model".
          enum: [model]
        owned_by:
          type: string
          description: The organization that owns the model.
      required:
        - id
        - object
        - created
        - owned_by
    ListModelsResponse:
      type: object
      description: Represents a list of models returned by the List models endpoint.
      properties:
        object:
          type: string
          description: The object type, which is always "list".
          enum: [list]
        data:
          type: array
          description: The list of models.
          items:
            $ref: "#/components/schemas/Model"
      required:
        - object
        - data
    DeleteModelResponse:
      type: object
      description: Represents a deleted response returned by the Delete model endpoint.
      properties:
        id:
          type: string
          description: The model identifier.
        deleted:
          type: boolean
          description: Whether the model was deleted.
        object:
          type: string
          description: The object type, which is always "model".
      required:
        - id
        - object
        - deleted
    CreateModerationRequest:
      type: object
      description: Request object for the Create moderation endpoint.
      properties:
        model:
          title: ModerationModel
          description: |
            Two content moderations models are available: `text-moderation-stable` and `text-moderation-latest`.

            The default is `text-moderation-latest` which will be automatically upgraded over time. This ensures you are always using our most accurate model. If you use `text-moderation-stable`, we will provide advanced notice before updating the model. Accuracy of `text-moderation-stable` may be slightly lower than for `text-moderation-latest`.
          default: "text-moderation-latest"
          example: "text-moderation-stable"
          anyOf:
            - type: string
              description: The ID of the model to use for this request.
            - type: string
              title: ModerationModels
              description: |
                Available moderation models. Mind that the list may not be exhaustive nor up-to-date.
              enum: [ "text-moderation-latest", "text-moderation-stable" ]
        input:
          title: ModerationInput
          description: The input text to classify
          oneOf:
            - type: string
              description: A string input.
              default: ""
              example: "I want to kill them."
            - type: array
              description: A list of string inputs.
              items:
                type: string
                default: ""
                example: "I want to kill them."
      required:
        - input
    CreateModerationResponse:
      type: object
      description: Represents policy compliance report by OpenAI's content moderation model against a given input.
      properties:
        id:
          type: string
          description: The unique identifier for the moderation request.
        model:
          type: string
          description: The model used to generate the moderation results.
        results:
          type: array
          description: A list of moderation objects.
          items:
            $ref: "#/components/schemas/Moderation"
      required:
        - id
        - model
        - results
    Moderation:
      type: object
      description: Represents policy compliance report by OpenAI's content moderation model against a given input.
      properties:
        flagged:
          type: boolean
          description: Whether the content violates [OpenAI's usage policies](https://platform.openai.com/policies/usage-policies).
        categories:
          $ref: "#/components/schemas/ModerationCategories"
        category_scores:
          $ref: "#/components/schemas/ModerationCategoriesScores"
      required:
        - flagged
        - categories
        - category_scores
    ModerationCategories:
      type: object
      description: A list of the categories, and whether they are flagged or not.
      properties:
        hate:
          type: boolean
          description: Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harrassment.
        hate/threatening:
          type: boolean
          description: Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.
        harassment:
          type: boolean
          description: Content that expresses, incites, or promotes harassing language towards any target.
        harassment/threatening:
          type: boolean
          description: Harassment content that also includes violence or serious harm towards any target.
        self-harm:
          type: boolean
          description: Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.
        self-harm/intent:
          type: boolean
          description: Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.
        self-harm/instructions:
          type: boolean
          description: Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.
        sexual:
          type: boolean
          description: Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).
        sexual/minors:
          type: boolean
          description: Sexual content that includes an individual who is under 18 years old.
        violence:
          type: boolean
          description: Content that depicts death, violence, or physical injury.
        violence/graphic:
          type: boolean
          description: Content that depicts death, violence, or physical injury in graphic detail.
      required:
        - hate
        - hate/threatening
        - harassment
        - harassment/threatening
        - self-harm
        - self-harm/intent
        - self-harm/instructions
        - sexual
        - sexual/minors
        - violence
        - violence/graphic
    ModerationCategoriesScores:
      type: object
      description: A list of the categories along with their scores as predicted by model.
      properties:
        hate:
          type: number
          description: The score for the category 'hate'.
        hate/threatening:
          type: number
          description: The score for the category 'hate/threatening'.
        harassment:
          type: number
          description: The score for the category 'harassment'.
        harassment/threatening:
          type: number
          description: The score for the category 'harassment/threatening'.
        self-harm:
          type: number
          description: The score for the category 'self-harm'.
        self-harm/intent:
          type: number
          description: The score for the category 'self-harm/intent'.
        self-harm/instructions:
          type: number
          description: The score for the category 'self-harm/instructions'.
        sexual:
          type: number
          description: The score for the category 'sexual'.
        sexual/minors:
          type: number
          description: The score for the category 'sexual/minors'.
        violence:
          type: number
          description: The score for the category 'violence'.
        violence/graphic:
          type: number
          description: The score for the category 'violence/graphic'.
      required:
        - hate
        - hate/threatening
        - harassment
        - harassment/threatening
        - self-harm
        - self-harm/intent
        - self-harm/instructions
        - sexual
        - sexual/minors
        - violence
        - violence/graphic

security:
  - ApiKeyAuth: [ ]
